
 1ï¸âƒ£ Test Organization

ğŸ‘‰ What it means: How testing work is structured in a project â€” roles, responsibilities, and processes.

 Example (Real-time): Think of testing like running a hospital.

   Doctors = Testers (diagnose the â€œbugsâ€)
   Head Doctor = Test Manager (organizes, assigns, ensures quality)
   Departments = Different testing types (functional, performance, security)

 In IT project:

   Test Manager â†’ decides who tests what, when, and with what tools.
   Testers â†’ execute test cases, report bugs.
   Developers â†’ fix bugs.
   Business Analyst â†’ explains requirements.

âœ… Test Organization ensures: Clear roles, no duplication, smooth workflow.

---

 2ï¸âƒ£ Independent Testing

ğŸ‘‰ What it means: Testing done by people different from the developers who built the code.

 Why? Developers may miss bugs in their own code (bias).

 Levels of independence:

   Lowest â†’ Developer tests own code.
   Medium â†’ Another developer tests.
   Higher â†’ Dedicated tester tests.
   Highest â†’ External test agency tests.

 Analogy: Imagine you wrote an exam paper. If you check your own paper, you may overlook mistakes. But if another teacher checks, errors are more likely to be caught.

 Example in IT:

   Developer builds a â€œLoginâ€ feature.
   Independent tester checks not only valid login but also invalid password, SQL injection, browser compatibility, etc.

âœ… Independent testing increases objectivity and defect detection.

---

 3ï¸âƒ£ Tasks of a Test Manager vs Tester

 Test Manager (Leader role)

   Defines the test strategy (overall approach).
   Plans resources, schedule, tools.
   Coordinates with dev team, business, clients.
   Tracks progress with metrics (defect density, test coverage).

  ğŸ‘‰ Example: Like a football coach â†’ decides training plan, positions, strategies.

 Tester (Executor role)

   Designs test cases.
   Executes tests.
   Reports bugs.
   Re-tests after fixes.

  ğŸ‘‰ Example: Like a football player â†’ follows the coachâ€™s plan and plays on the field.

âœ… Together â†’ Manager = Planner, Tester = Doer.

---

 4ï¸âƒ£ Test Planning and Estimation

ğŸ‘‰ Test Planning: Document that describes how testing will be done.
ğŸ‘‰ Estimation: Predicting how much effort/time/cost testing will take.

 Analogy: Planning a wedding.

   Test Plan = Decide venue, food, decorations, guest list.
   Estimation = How many days to arrange, how much money, how many people needed.

 Example in IT:

   Project: Banking app.
   Plan: What types of testing? (functional, security, performance).
   Estimation: 3 testers for 2 months = 300 test cases = 6 weeks execution.

âœ… Test Planning ensures clarity, Estimation ensures realistic deadlines.

---

 5ï¸âƒ£ Purpose and Content of a Test Plan

ğŸ‘‰ Purpose: To give a roadmap of testing so everyone knows scope, strategy, resources, schedule, risks.

ğŸ‘‰ Content of a Test Plan (common structure in ISTQB & IEEE 829):

1. Introduction â†’ Why testing is needed.
2. Test Items â†’ What features/modules will be tested.
3. Scope (In & Out) â†’ What is included/excluded.
4. Test Approach/Strategy â†’ Manual vs automation, levels of testing.
5. Test Environment â†’ Hardware, software, browsers.
6. Resources & Roles â†’ Test manager, testers, tools.
7. Schedule & Milestones â†’ When to start, end, deadlines.
8. Entry/Exit Criteria â†’ When to start testing, when to stop.
9. Risks & Mitigation â†’ If test environment not ready, whatâ€™s backup.
10. Deliverables â†’ Test cases, reports, metrics.

 Example in Real-time (Bank App Test Plan):

   Scope: Login, signup, funds transfer (in scope); AI chatbot (out of scope).
   Environment: Windows 10, Chrome, DB = MySQL.
   Entry criteria: Dev provides stable build.
   Exit criteria: All high severity defects fixed, 95% pass rate.
   Risks: If UAT environment not ready â†’ delay testing by 1 week.

1ï¸âƒ£ Test Strategy and Test Approach

 ğŸ”¹ Test Strategy

ğŸ‘‰ High-level plan for testing across the whole project. Itâ€™s long-term and usually written once for the entire organization/project.

 Defines what types of testing (functional, regression, performance, security).
 Explains tools, levels, roles.
 Written by: Test Manager or QA Lead.

ğŸ›  Analogy: Like a countryâ€™s defense strategy â†’ Defines whether to use army, navy, or air force in different situations.

ğŸ›  Example (IT):

 Banking app â†’ Strategy includes:

   Functional testing for login & funds transfer.
   Security testing for fraud detection.
   Automation using Selenium for regression.

---

 ğŸ”¹ Test Approach

ğŸ‘‰ Detailed implementation of the strategy at project or sprint level.

 Defines how tests will be carried out.
 Written by: Test Lead / Senior Tester.

ğŸ›  Analogy: If strategy is the â€œwhatâ€, then approach is the â€œhow.â€

 Strategy = â€œWe will play cricket.â€
 Approach = â€œWeâ€™ll use 3 bowlers, 7 batsmen, 1 all-rounder.â€

ğŸ›  Example (IT):

 Login module â†’ Approach = Write 15 manual test cases, automate 10 using Selenium, test on Chrome & Firefox.

âœ… Difference:

 Strategy = Big picture (long-term, organizational level).
 Approach = Execution detail (short-term, sprint/project level).

---

 2ï¸âƒ£ Entry Criteria and Exit Criteria

 ğŸ”¹ Entry Criteria (Definition of Ready â€“ DoR)

ğŸ‘‰ Conditions that must be met before testing can start.

ğŸ›  Analogy: You canâ€™t start cooking ğŸ³ unless ingredients are ready.

ğŸ›  Example (IT):

 Software is ready.
 Test cases are written and reviewed.
 Test data is prepared.
 Tools (e.g., Selenium, JIRA) are available.

âœ… If entry criteria not met â†’ testing may fail or give false results.

---

 ğŸ”¹ Exit Criteria (Definition of Done â€“ DoD)

ğŸ‘‰ Conditions that must be met before testing can end / feature is considered done.

ğŸ›  Analogy: You canâ€™t say the dish is ready until it is fully cooked and served ğŸ².

ğŸ›  Example (IT):

 All planned test cases executed.
 95% test cases passed.
 No critical/high-severity bugs remain open.
 Test summary report created.

âœ… If exit criteria not met â†’ product cannot be released.

---

 3ï¸âƒ£ Test Execution Schedule

ğŸ‘‰ It is a timeline/calendar showing when each test will be executed, by whom, and in what order.

ğŸ›  Analogy: Like a train timetable ğŸš† â†’ defines when each train (test) runs and whoâ€™s in charge.

ğŸ›  Example (IT):
For an E-commerce project sprint (2 weeks):

 Day 1-2 â†’ Smoke testing.
 Day 3-5 â†’ Functional testing of Login, Search.
 Day 6-8 â†’ Checkout & Payment.
 Day 9-10 â†’ Regression testing.
 Day 11 â†’ Performance testing.
 Day 12-13 â†’ Bug re-testing.
 Day 14 â†’ Final test summary report.

âœ… Helps in resource planning and avoids last-minute chaos.

---

 4ï¸âƒ£ Factors Influencing Test Effort

ğŸ‘‰ Test effort = amount of work/time needed for testing.

Factors include:

1. Size of the system â†’ More features = more test cases.
2. Complexity â†’ Login is simple, fraud detection is complex.
3. Quality of requirements â†’ Clear requirements â†’ less effort.
4. Tools & Automation â†’ Automation reduces effort in long run.
5. Team skill level â†’ Experienced testers work faster.
6. Environment availability â†’ If test servers are unstable, effort increases.
7. Dependencies â†’ If developers delay builds, testers get less time.

ğŸ›  Analogy: Like planning a wedding ğŸ’ â†’ effort depends on number of guests, type of venue, weather, and how skilled the event manager is.

ğŸ›  Example (IT):

 Testing a simple calculator app = 20 test cases â†’ low effort.
 Testing a banking app with loans, accounts, transactions, payments = 2000 test cases â†’ huge effort.

---

 5ï¸âƒ£ Test Estimation Techniques

ğŸ‘‰ Test Estimation = Predicting how much time, effort, people are needed for testing.

 Common Techniques:

1. Expert Judgment

 Based on experience of senior testers.
 ğŸ›  Example: Tester says, â€œI tested similar login before, will take 2 days.â€

2. Work Breakdown Structure (WBS)

 Break testing into smaller tasks, estimate each, then sum up.
 ğŸ›  Example:

   Write test cases â†’ 2 days
   Execute test cases â†’ 4 days
   Re-test bugs â†’ 2 days
   Total = 8 days

3. Function Point Analysis

 Estimate based on number of functionalities and their complexity.
 ğŸ›  Example: Login = 5 points, Checkout = 20 points â†’ bigger = more effort.

4. Test Case Point Method

 Count number of test cases and effort per case.
 ğŸ›  Example: 100 cases Ã— 15 mins each = 25 hours.

5. Three-point Estimation (PERT)

 Uses Optimistic (O), Pessimistic (P), and Most Likely (M).
 Formula: (O + 4M + P) Ã· 6.
 ğŸ›  Example: For a module â†’ O=5 days, M=7 days, P=11 days â†’ Estimation = 7.3 days.

ğŸ›  Analogy: Like estimating how long cooking will take.

 If everything goes smooth â†’ 30 mins.
 If a small problem â†’ 45 mins.
 If big problem â†’ 1 hour.
 Average â†’ \~45 mins.

---


 1ï¸âƒ£ Test Monitoring and Control

 ğŸ”¹ Test Monitoring

ğŸ‘‰ Tracking and checking testing progress against the plan.

 It answers: â€œWhere are we right now?â€

ğŸ›  Analogy: Like checking the Google Maps GPS while driving ğŸš— â†’ Are we still on the right route? How much distance left?

ğŸ›  Example (IT):

 Planned 100 test cases for this sprint.
 By Day 5, only 30 are executed â†’ monitoring shows we are behind schedule.

---

 ğŸ”¹ Test Control

ğŸ‘‰ Actions taken to correct deviations when monitoring shows issues.

 It answers: â€œWhat do we do if we are off-track?â€

ğŸ›  Analogy: If GPS shows a traffic jam ğŸš¦, you take a different route.

ğŸ›  Example (IT):

 Only 30 test cases executed instead of 50.
 Control action = Assign 2 more testers OR reduce low-priority tests.


 2ï¸âƒ£ Metrics Used in Testing

ğŸ‘‰ Metrics = Numbers that help measure quality and progress.

 Common Metrics:

1. Test Progress Metrics

 % of test cases executed.
 % of test cases passed/failed.

2. Defect Metrics

 Defect Density = No. of defects Ã· size of module.
 Defect Severity Distribution = High/Medium/Low defects.
 Defect Removal Efficiency = % of defects found before release.

3. Effort Metrics

 Planned effort vs. actual effort.
 Test execution rate (e.g., 20 cases/day).

ğŸ›  Analogy: Like a fitness tracker ğŸƒ â€“ counts steps, calories, heart rate â†’ gives you data to judge progress.

ğŸ›  Example (IT):

 Sprint Goal = 100 test cases.
 Day 7 = 70 executed, 50 passed, 20 failed.
 10 defects found (4 critical, 6 medium).

---

 3ï¸âƒ£ Purposes of Test Reports

ğŸ‘‰ Test Report = Summary of testing activities, results, and product quality.

Why we need them?

1. To inform stakeholders (managers, clients, devs).
2. To show product quality & readiness.
3. To track project risks.
4. To improve future projects (lessons learned).

---

 4ï¸âƒ£ Contents of a Test Report

Typical sections include:

1. Introduction â†’ What was tested (scope, features, environment).
2. Test Progress â†’ Planned vs. executed test cases.
3. Defect Summary â†’ Open, closed, severity, priority.
4. Test Results â†’ Pass %, Fail %, Blocked.
5. Risks/Issues â†’ Any problems (e.g., unstable environment).
6. Exit Criteria Check â†’ Were all conditions met?
7. Conclusion/Recommendation â†’ Ready for release or not.

---

 5ï¸âƒ£ Audiences for Test Reports

Different people care about different parts of the report:

 Project Manager â†’ Wants status vs. schedule, risks, overall readiness.
 Developers â†’ Want detailed defect info (logs, reproduction steps).
 Clients/Business â†’ Want summary: Is the product ready? Is quality acceptable?
 QA Team â†’ Want insights for process improvement.

ğŸ›  Analogy: Like a school report card ğŸ“˜:

 Parents â†’ look at overall grades.
 Teacher â†’ looks at detailed marks and weak areas.
 Student â†’ sees progress and where to improve.

ğŸ›  Example (IT):
For an e-commerce checkout module:

 200 test cases planned, 190 executed (95%).
 160 passed, 30 failed.
 5 critical defects remain open.
 Recommendation: Do not release until critical bugs are fixed.

---


 1ï¸âƒ£ Definition of Risk

ğŸ‘‰ Risk = Uncertainty of something going wrong in the future.
It is the possibility of a negative event that may affect the project or product.

 Risk always has:

  1. Cause â†’ Why it could happen.
  2. Event â†’ What might happen.
  3. Impact â†’ How bad it will be if it happens.

ğŸ›  Analogy:
Planning a wedding outdoors ğŸŒ§ï¸ â†’

 Cause = Rainy season.
 Event = It may rain on wedding day.
 Impact = Guests get wet, wedding delayed.

ğŸ›  Example (IT):

 Cause = New developer joins.
 Event = Code might have bugs.
 Impact = More defects in login feature.

---

 2ï¸âƒ£ Types of Risks

 ğŸ”¹ Product Risks

ğŸ‘‰ Problems in the quality of the product itself.

ğŸ›  Analogy: Car ğŸš— has weak brakes â†’ risk of accidents.

ğŸ›  Examples (IT):

 Login feature may not work with special characters.
 Payment gateway may fail under high load.
 Data may not be encrypted â†’ security risk.

---

 ğŸ”¹ Project Risks

ğŸ‘‰ Problems in the way the project is run/managed.

ğŸ›  Analogy: Wedding planner forgets to book the hall â†’ event delayed.

ğŸ›  Examples (IT):

 Test environment not ready on time.
 Budget cuts reduce tester team size.
 Developer delays in delivering build.

âœ… Summary:

 Product Risks = Defects in the software.
 Project Risks = Issues in project management or resources.

---

 3ï¸âƒ£ Risk-based Testing

ğŸ‘‰ Testing where you focus more effort on high-risk areas of the product.

ğŸ›  Analogy:

 In airport security âœˆï¸ â†’ They check international passengers more strictly than domestic ones, because risk is higher.

ğŸ›  Example (IT):

 In a banking app:

   High-risk = Payment transfer (money loss if it fails). â†’ Test deeply with functional, security, performance.
   Low-risk = Change profile picture. â†’ Test lightly.

âœ… Benefit â†’ Ensures critical areas are tested first and most thoroughly.

---

 4ï¸âƒ£ Risks and Product Quality

ğŸ‘‰ If risks are not managed properly, product quality suffers.

ğŸ›  Examples (IT):

 If performance risks ignored â†’ app may crash during big sales.
 If security risks ignored â†’ hackers may steal data.
 If test environment risks ignored â†’ testers canâ€™t test properly, leading to undetected bugs.

ğŸ›  Analogy: Ignoring health risks (like not exercising, eating junk) â†’ long-term quality of life goes down.

âœ… So managing risks = ensuring good quality product.


# 1ï¸âƒ£ Defect Management

ğŸ‘‰ Defect Management = The process of finding, fixing, tracking, and closing bugs.
It ensures defects are handled in a systematic way so nothing slips through.

---

 ğŸ”¹ Defect Life Cycle (Bug Life Cycle)

When a bug is reported, it goes through stages:

1. New / Open â†’ Tester finds bug in login and reports it in JIRA.
2. Assigned â†’ Bug is assigned to a developer.
3. In Progress â†’ Developer starts fixing it.
4. Fixed / Resolved â†’ Developer fixes bug and marks as resolved.
5. Retesting â†’ Tester checks again in new build.
6. Closed â†’ If bug is fixed â†’ status closed.
7. Reopened â†’ If bug still exists â†’ reopened.

ğŸ›  Analogy: Like complaining about a broken TV ğŸ“º in a store.

 You report issue â†’ Customer care assigns technician â†’ Technician fixes â†’ You check â†’ If fine, case closed.

---

 ğŸ”¹ Defect Severity vs Priority

 Severity = How badly the defect impacts system.
 Priority = How soon it must be fixed.

ğŸ›  Example (E-commerce app):

 High Severity, High Priority â†’ â€œPay Nowâ€ button not working.
 High Severity, Low Priority â†’ Crash happens only on an old browser.
 Low Severity, High Priority â†’ Company logo spelled wrong â†’ Bad for brand.
 Low Severity, Low Priority â†’ Minor color mismatch on footer.

---

 ğŸ”¹ Why Defect Management is Important?

 Tracks all issues in one place (JIRA, Bugzilla, Azure DevOps).
 Helps prioritize fixes.
 Ensures quality before release.
 Gives metrics for process improvement.

---

# 2ï¸âƒ£ Testing Metrics

ğŸ‘‰ Metrics = Numbers that measure how testing is going (progress) and product quality.

---

 ğŸ”¹ Common Testing Metrics

# 1. Test Case Metrics

 Planned vs Executed â†’ No. of cases executed / planned.
 Pass % â†’ Passed Ã· Executed.
 Fail % â†’ Failed Ã· Executed.

ğŸ›  Example:

 Planned = 100 cases, Executed = 80, Pass = 60, Fail = 20.
 Execution % = 80%. Pass % = 75%.

---

# 2. Defect Metrics

 Defect Density = Defects Ã· size (e.g., per 1,000 LOC or per module).
 Defect Leakage = Defects missed in testing but found later (in UAT/production).
 Defect Removal Efficiency = (Defects found in testing Ã· Total defects) Ã— 100.

ğŸ›  Example:

 50 defects found in QA, 10 defects found by client later.
 Leakage = 10. DRE = 50 Ã· (50+10) = 83%.

---

# 3. Effort Metrics

 Planned vs Actual Effort (hours/days).
 Test Execution Rate (cases/day).

ğŸ›  Example:

 Planned = 5 testers Ã— 5 days = 25 days.
 Actual = 30 days. â†’ Overrun = +5 days.

---

# 4. Coverage Metrics

 Requirements Coverage â†’ % of requirements with test cases.
 Code Coverage â†’ % of code executed during testing.

ğŸ›  Example:

 50 requirements, 45 covered â†’ 90% coverage.

---

 ğŸ”¹ Why Metrics are Needed?

 To monitor progress (Are we on track?).
 To control testing (Need more resources?).
 To show quality to stakeholders (Is product ready?).
 To improve future projects (Lessons from metrics).

